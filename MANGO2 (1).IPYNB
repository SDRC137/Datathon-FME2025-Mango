{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8417bf6",
   "metadata": {},
   "source": [
    "# Script de Submission Múltiple\n",
    "\n",
    "Este notebook genera múltiples submissions con diferentes factores de riesgo para optimizar las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255ab3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import lightgbm as lgb\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59d8979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de archivos y parámetros\n",
    "TRAIN_FILE = \"train.csv\"\n",
    "TEST_FILE = \"test.csv\"\n",
    "\n",
    "# Pon el 'best_iteration' que encontraste en el Script 1\n",
    "N_ESTIMATORS_OPTIMO = 115 # (Este es un EJEMPLO)\n",
    "\n",
    "N_NEIGHBORS = 5\n",
    "\n",
    "# Define tus 8 \"Factores de Riesgo\" \n",
    "RISK_FACTORS = {\n",
    "    'intento_1': 30,\n",
    "    'intento_2': 20,\n",
    "    'intento_3': 25,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95986145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones auxiliares\n",
    "def parse_embedding(embedding_str):\n",
    "    if pd.isna(embedding_str): return np.zeros(512) \n",
    "    try:\n",
    "        cleaned_str = re.sub(r\"[^0-9.,-]\", \"\", str(embedding_str))\n",
    "        values = [float(x) for x in cleaned_str.split(',') if x]\n",
    "        return np.array(values) if len(values) == 512 else np.zeros(512)\n",
    "    except Exception as e: return np.zeros(512)\n",
    "\n",
    "def preprocess_features_lgbm(df, categorical_features):\n",
    "    df['phase_in'] = pd.to_datetime(df['phase_in'], errors='coerce', dayfirst=True)\n",
    "    df['phase_in_month'] = df['phase_in'].dt.month.fillna(0).astype(int)\n",
    "    df['phase_in_week'] = df['phase_in'].dt.isocalendar().week.fillna(0).astype('UInt32')\n",
    "    bool_cols = df.select_dtypes(include=['bool']).columns\n",
    "    for col in bool_cols: df[col] = df[col].fillna(False)\n",
    "    for col in df.columns:\n",
    "        if col in categorical_features:\n",
    "            df[col] = df[col].fillna(\"Missing\").astype('category')\n",
    "        elif col in df.select_dtypes(include=np.number).columns:\n",
    "            df[col] = df[col].fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e798a44f",
   "metadata": {},
   "source": [
    "## Paso 1: Cargar y limpiar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d9dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos\n",
    "print(f\"Paso 1: Cargando {TRAIN_FILE} y {TEST_FILE}...\")\n",
    "train_df = pd.read_csv(TRAIN_FILE, delimiter=';')\n",
    "test_df = pd.read_csv(TEST_FILE, delimiter=';')\n",
    "test_df = test_df.loc[:, ~test_df.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6026843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing - Eliminando columnas con >40% missings\n",
    "print(\"Paso 1.1: Preprocessing - Eliminando columnas con >40% missings...\")\n",
    "\n",
    "# Identificar y eliminar columnas con >40% de valores faltantes en train\n",
    "missing_pct = train_df.isnull().sum() / len(train_df)\n",
    "cols_to_drop = missing_pct[missing_pct > 0.40].index.tolist()\n",
    "print(f\"Columnas eliminadas por >40% missings: {cols_to_drop}\")\n",
    "\n",
    "train_df = train_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "test_df = test_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "# Guardar dimensiones originales\n",
    "original_train_len = len(train_df)\n",
    "print(f\"Filas originales train: {original_train_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573e7778",
   "metadata": {},
   "source": [
    "## Paso 2: Agregar datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fdcc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar datos de entrenamiento por ID\n",
    "print(\"Paso 2: Agregando datos de entrenamiento por ID...\")\n",
    "agg_ops = {\n",
    "    'weekly_demand': 'sum', 'weekly_sales': 'sum', 'Production': 'first',\n",
    "    'image_embedding': 'first', 'num_stores': 'first', 'num_sizes': 'first',\n",
    "    'price': 'mean', 'life_cycle_length': 'first', 'phase_in': 'first',\n",
    "    'aggregated_family': 'first', 'family': 'first', 'category': 'first', 'fabric': 'first',\n",
    "    'color_name': 'first', 'length_type': 'first', 'silhouette_type': 'first',\n",
    "    'waist_type': 'first', 'neck_lapel_type': 'first', 'sleeve_length_type': 'first',\n",
    "    'woven_structure': 'first', 'knit_structure': 'first', 'print_type': 'first',\n",
    "    'archetype': 'first', 'moment': 'first', 'has_plus_sizes': 'first'\n",
    "}\n",
    "agg_ops = {col: op for col, op in agg_ops.items() if col in train_df.columns}\n",
    "train_agg_df = train_df.groupby('ID').agg(agg_ops).reset_index()\n",
    "train_agg_df.rename(columns={'weekly_demand': 'total_demand', 'weekly_sales': 'total_sales'}, inplace=True)\n",
    "train_agg_df['sell_through'] = train_agg_df['total_sales'] / (train_agg_df['Production'] + 1e-6) \n",
    "train_agg_df['sell_through'] = train_agg_df['sell_through'].fillna(0)\n",
    "train_agg_df['is_stockout'] = (train_agg_df['sell_through'] >= 0.98).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429f4590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar filas con valores faltantes en columnas clave\n",
    "print(\"Paso 2.1: Eliminando filas con valores faltantes en columnas clave...\")\n",
    "cols_critical = ['Production', 'image_embedding']  # Columnas que no pueden tener NaN\n",
    "train_agg_df = train_agg_df.dropna(subset=[c for c in cols_critical if c in train_agg_df.columns])\n",
    "print(f\"Filas después de eliminar missings críticos: {len(train_agg_df)} (eliminadas: {original_train_len - len(train_agg_df)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33ce844",
   "metadata": {},
   "source": [
    "## Paso 3: Procesamiento de embeddings y k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a564d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesamiento de embeddings\n",
    "print(\"Paso 3: Procesando embeddings y creando features k-NN...\")\n",
    "train_agg_df['embedding_vec'] = train_agg_df['image_embedding'].apply(parse_embedding)\n",
    "test_df['embedding_vec'] = test_df['image_embedding'].apply(parse_embedding)\n",
    "X_train_embeddings = np.stack(train_agg_df['embedding_vec'].values)\n",
    "X_test_embeddings = np.stack(test_df['embedding_vec'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edfea41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear modelo k-NN y obtener vecinos\n",
    "knn_model = NearestNeighbors(n_neighbors=N_NEIGHBORS, metric='cosine', n_jobs=-1)\n",
    "knn_model.fit(X_train_embeddings)\n",
    "distances, indices = knn_model.kneighbors(X_test_embeddings)\n",
    "\n",
    "cols_to_fetch = {\n",
    "    'total_demand': 'total_demand', 'total_sales': 'total_sales', 'Production': 'Production', \n",
    "    'num_stores': 'num_stores', 'sell_through': 'sell_through', 'is_stockout': 'is_stockout'\n",
    "}\n",
    "for col_name_in_train, new_feature_prefix in cols_to_fetch.items():\n",
    "    if col_name_in_train in train_agg_df.columns:\n",
    "        neighbor_values = train_agg_df.iloc[indices.flatten()][col_name_in_train].values\n",
    "        neighbor_values = neighbor_values.reshape(len(test_df), N_NEIGHBORS)\n",
    "        test_df[f'avg_neighbor_{new_feature_prefix}'] = np.mean(neighbor_values, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bc6a0a",
   "metadata": {},
   "source": [
    "## Paso 4: Pre-procesamiento final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd0e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para One-Hot Encoding\n",
    "print(\"Paso 4: Pre-procesamiento final - One-Hot Encoding y Escalado...\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "special_cols = ['image_embedding', 'embedding_vec', 'phase_in', 'phase_out', 'color_rgb', 'total_demand', 'total_sales', 'ID']\n",
    "all_categorical = list(set(train_agg_df.select_dtypes(include=['object']).columns) | set(test_df.select_dtypes(include=['object']).columns))\n",
    "all_categorical = [col for col in all_categorical if col not in special_cols]\n",
    "\n",
    "train_shape = len(train_agg_df)\n",
    "combined_df = pd.concat([train_agg_df.drop(['total_demand', 'total_sales'], axis=1, errors='ignore').copy(), test_df.copy()], ignore_index=True)\n",
    "\n",
    "# Preprocesar fechas y básicos\n",
    "combined_df = preprocess_features_lgbm(combined_df, all_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a5e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding\n",
    "print(f\"Aplicando One-Hot Encoding a {len(all_categorical)} columnas categóricas...\")\n",
    "# Convertir categóricas a strings primero\n",
    "for col in all_categorical:\n",
    "    if col in combined_df.columns:\n",
    "        combined_df[col] = combined_df[col].astype(str)\n",
    "\n",
    "# One-hot encoding\n",
    "combined_df = pd.get_dummies(combined_df, columns=all_categorical, drop_first=True, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812ff216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza de nombres de columnas para LightGBM\n",
    "# LightGBM no acepta caracteres especiales JSON en nombres\n",
    "combined_df.columns = combined_df.columns.str.replace('[', '_', regex=False)\n",
    "combined_df.columns = combined_df.columns.str.replace(']', '_', regex=False)\n",
    "combined_df.columns = combined_df.columns.str.replace('<', '_', regex=False)\n",
    "combined_df.columns = combined_df.columns.str.replace('>', '_', regex=False)\n",
    "combined_df.columns = combined_df.columns.str.replace('{', '_', regex=False)\n",
    "combined_df.columns = combined_df.columns.str.replace('}', '_', regex=False)\n",
    "combined_df.columns = combined_df.columns.str.replace('\"', '_', regex=False)\n",
    "combined_df.columns = combined_df.columns.str.replace(\"'\", '_', regex=False)\n",
    "combined_df.columns = combined_df.columns.str.replace(':', '_', regex=False)\n",
    "combined_df.columns = combined_df.columns.str.replace(',', '_', regex=False)\n",
    "combined_df.columns = combined_df.columns.str.replace(' ', '_', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d375fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar nuevamente train y test\n",
    "train_processed_df = combined_df.iloc[:train_shape].copy()\n",
    "test_processed_df = combined_df.iloc[train_shape:].copy()\n",
    "train_processed_df = train_processed_df.assign(total_demand=train_agg_df['total_demand'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52abebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado de datos numéricos\n",
    "print(\"Escalando features numéricas con StandardScaler...\")\n",
    "target = 'total_demand'\n",
    "features = [col for col in train_processed_df.columns if col not in special_cols and col != target]\n",
    "\n",
    "# Identificar columnas numéricas (excluyendo one-hot encoded que son 0/1)\n",
    "numeric_cols = train_processed_df[features].select_dtypes(include=[np.number]).columns.tolist()\n",
    "# Filtrar one-hot (típicamente tienen valores solo 0 y 1)\n",
    "numeric_cols_to_scale = [c for c in numeric_cols if train_processed_df[c].nunique() > 2]\n",
    "\n",
    "if numeric_cols_to_scale:\n",
    "    scaler = StandardScaler()\n",
    "    train_processed_df[numeric_cols_to_scale] = scaler.fit_transform(train_processed_df[numeric_cols_to_scale])\n",
    "    test_processed_df[numeric_cols_to_scale] = scaler.transform(test_processed_df[numeric_cols_to_scale])\n",
    "    print(f\"Escaladas {len(numeric_cols_to_scale)} columnas numéricas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cc83b8",
   "metadata": {},
   "source": [
    "## Paso 5: Definir features y target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dae284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir features y target\n",
    "print(\"Paso 5: Definiendo features y target...\")\n",
    "target = 'total_demand'\n",
    "features = [col for col in train_processed_df.columns if col not in special_cols and col != target]\n",
    "# Ya no necesitamos categorical features porque hicimos one-hot encoding\n",
    "X_train_final = train_processed_df[features]\n",
    "y_train_final = train_processed_df[target]\n",
    "X_test_final = test_processed_df[features]\n",
    "X_test_final = X_test_final[X_train_final.columns]\n",
    "print(f\"Usando {len(features)} features (incluyendo one-hot encoded).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc2d5e6",
   "metadata": {},
   "source": [
    "## Paso 6: Entrenar modelo final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbc58c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelo LightGBM final con 100% de datos\n",
    "print(f\"Paso 6: Entrenando LightGBM final con 100% de datos...\")\n",
    "print(f\"Parámetros: objective='regression_l1', n_estimators={N_ESTIMATORS_OPTIMO}\")\n",
    "lgbm_params = {\n",
    "    'objective': 'regression_l1', 'metric': 'mae',\n",
    "    'n_estimators': N_ESTIMATORS_OPTIMO, 'learning_rate': 0.05,\n",
    "    'n_jobs': -1, 'random_state': 42, 'verbose': -1\n",
    "}\n",
    "model_final = lgb.LGBMRegressor(**lgbm_params)\n",
    "# Sin categorical_feature porque ya hicimos one-hot encoding\n",
    "model_final.fit(X_train_final, y_train_final)\n",
    "print(\"Entrenamiento final completado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31b91c0",
   "metadata": {},
   "source": [
    "## Paso 7: Generar submissions con diferentes factores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0448a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar predicciones base\n",
    "print(\"Paso 7: Generando predicciones base (MAE)...\")\n",
    "# Generar UNA SOLA VEZ las predicciones de \"precisión\"\n",
    "base_predictions = model_final.predict(X_test_final)\n",
    "base_predictions[base_predictions < 0] = 0 # Limpiar negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5192cfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar archivos de submission con diferentes factores\n",
    "print(f\"¡Generando {len(RISK_FACTORS)} archivos de submission!\")\n",
    "for (name, factor) in RISK_FACTORS.items():\n",
    "    \n",
    "    # Aplicar el \"Factor de Riesgo\"\n",
    "    final_predictions = base_predictions * factor\n",
    "    \n",
    "    submission_df = pd.DataFrame({'ID': test_df['ID'], 'Production': final_predictions})\n",
    "    submission_df['Production'] = submission_df['Production'].round().astype(int)\n",
    "    \n",
    "    filename = f\"submission_{name}_factor_{factor:.2f}.csv\"\n",
    "    submission_df.to_csv(filename, index=False)\n",
    "    \n",
    "    print(f\"Archivo guardado: {filename}\")\n",
    "    print(submission_df.head())\n",
    "    print(\"---\")\n",
    "print(f\"{len(RISK_FACTORS)} archivos de submission generados. ¡Súbelos y compara los scores!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
