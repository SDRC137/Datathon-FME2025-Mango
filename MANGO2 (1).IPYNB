{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11d5d491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso 1: Cargando train.csv y test.csv...\n",
      "Paso 1.1: Preprocessing - Eliminando columnas con >40% missings...\n",
      "Columnas eliminadas por >40% missings: ['waist_type', 'heel_shape_type', 'toecap_type', 'knit_structure', 'archetype']\n",
      "Filas originales train: 95339\n",
      "Paso 2: Agregando datos de entrenamiento por ID...\n",
      "Paso 2.1: Eliminando filas con valores faltantes en columnas clave...\n",
      "Filas después de eliminar missings críticos: 9843 (eliminadas: 85496)\n",
      "Paso 3: Procesando embeddings y creando features k-NN...\n",
      "Paso 4: Pre-procesamiento final - One-Hot Encoding y Escalado...\n",
      "Aplicando One-Hot Encoding a 12 columnas categóricas...\n",
      "Escalando features numéricas con StandardScaler...\n",
      "Escaladas 8 columnas numéricas\n",
      "Paso 5: Definiendo features y target...\n",
      "Usando 280 features (incluyendo one-hot encoded).\n",
      "Paso 6: Entrenando LightGBM final con 100% de datos...\n",
      "Parámetros: objective='regression_l1', n_estimators=115\n",
      "Entrenamiento final completado.\n",
      "Paso 7: Generando predicciones base (MAE)...\n",
      "¡Generando 3 archivos de submission!\n",
      "Archivo guardado: submission_intento_1_factor_30.00.csv\n",
      "    ID  Production\n",
      "0   90       10589\n",
      "1   16       29632\n",
      "2   65       30336\n",
      "3  138        9096\n",
      "4  166        7059\n",
      "---\n",
      "Archivo guardado: submission_intento_2_factor_20.00.csv\n",
      "    ID  Production\n",
      "0   90        7059\n",
      "1   16       19755\n",
      "2   65       20224\n",
      "3  138        6064\n",
      "4  166        4706\n",
      "---\n",
      "Archivo guardado: submission_intento_3_factor_25.00.csv\n",
      "    ID  Production\n",
      "0   90        8824\n",
      "1   16       24693\n",
      "2   65       25280\n",
      "3  138        7580\n",
      "4  166        5883\n",
      "---\n",
      "3 archivos de submission generados. ¡Súbelos y compara los scores!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SCRIPT DE SUBMISSION MÚLTIPLE (USA TUS 8 INTENTOS)\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "\n",
    "TRAIN_FILE = \"train.csv\"\n",
    "TEST_FILE = \"test.csv\"\n",
    "\n",
    "\n",
    "# Pon el 'best_iteration' que encontraste en el Script 1\n",
    "N_ESTIMATORS_OPTIMO = 115 # (Este es un EJEMPLO))\n",
    "# ---\n",
    "\n",
    "N_NEIGHBORS = 5\n",
    "\n",
    "# Define tus 8 \"Factores de Riesgo\" \n",
    "RISK_FACTORS = {\n",
    "    'intento_1': 30,\n",
    "    'intento_2': 20,\n",
    "    'intento_3': 25,\n",
    "\n",
    "}\n",
    "\n",
    "# --- (Funciones parse_embedding y preprocess_features_lgbm) ---\n",
    "def parse_embedding(embedding_str):\n",
    "    if pd.isna(embedding_str): return np.zeros(512) \n",
    "    try:\n",
    "        cleaned_str = re.sub(r\"[^0-9.,-]\", \"\", str(embedding_str))\n",
    "        values = [float(x) for x in cleaned_str.split(',') if x]\n",
    "        return np.array(values) if len(values) == 512 else np.zeros(512)\n",
    "    except Exception as e: return np.zeros(512)\n",
    "def preprocess_features_lgbm(df, categorical_features):\n",
    "    df['phase_in'] = pd.to_datetime(df['phase_in'], errors='coerce', dayfirst=True)\n",
    "    df['phase_in_month'] = df['phase_in'].dt.month.fillna(0).astype(int)\n",
    "    df['phase_in_week'] = df['phase_in'].dt.isocalendar().week.fillna(0).astype('UInt32')\n",
    "    bool_cols = df.select_dtypes(include=['bool']).columns\n",
    "    for col in bool_cols: df[col] = df[col].fillna(False)\n",
    "    for col in df.columns:\n",
    "        if col in categorical_features:\n",
    "            df[col] = df[col].fillna(\"Missing\").astype('category')\n",
    "        elif col in df.select_dtypes(include=np.number).columns:\n",
    "            df[col] = df[col].fillna(0)\n",
    "    return df\n",
    "# ---\n",
    "\n",
    "# --- 1. Cargar Datos ---\n",
    "print(f\"Paso 1: Cargando {TRAIN_FILE} y {TEST_FILE}...\")\n",
    "train_df = pd.read_csv(TRAIN_FILE, delimiter=';')\n",
    "test_df = pd.read_csv(TEST_FILE, delimiter=';')\n",
    "test_df = test_df.loc[:, ~test_df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# --- 1.1 PREPROCESSING ROBUSTO ---\n",
    "print(\"Paso 1.1: Preprocessing - Eliminando columnas con >40% missings...\")\n",
    "\n",
    "# Identificar y eliminar columnas con >40% de valores faltantes en train\n",
    "missing_pct = train_df.isnull().sum() / len(train_df)\n",
    "cols_to_drop = missing_pct[missing_pct > 0.40].index.tolist()\n",
    "print(f\"Columnas eliminadas por >40% missings: {cols_to_drop}\")\n",
    "\n",
    "train_df = train_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "test_df = test_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "# Guardar dimensiones originales\n",
    "original_train_len = len(train_df)\n",
    "print(f\"Filas originales train: {original_train_len}\")\n",
    "\n",
    "# --- 2. Agregar Datos de Entrenamiento ---\n",
    "print(\"Paso 2: Agregando datos de entrenamiento por ID...\")\n",
    "agg_ops = {\n",
    "    'weekly_demand': 'sum', 'weekly_sales': 'sum', 'Production': 'first',\n",
    "    'image_embedding': 'first', 'num_stores': 'first', 'num_sizes': 'first',\n",
    "    'price': 'mean', 'life_cycle_length': 'first', 'phase_in': 'first',\n",
    "    'aggregated_family': 'first', 'family': 'first', 'category': 'first', 'fabric': 'first',\n",
    "    'color_name': 'first', 'length_type': 'first', 'silhouette_type': 'first',\n",
    "    'waist_type': 'first', 'neck_lapel_type': 'first', 'sleeve_length_type': 'first',\n",
    "    'woven_structure': 'first', 'knit_structure': 'first', 'print_type': 'first',\n",
    "    'archetype': 'first', 'moment': 'first', 'has_plus_sizes': 'first'\n",
    "}\n",
    "agg_ops = {col: op for col, op in agg_ops.items() if col in train_df.columns}\n",
    "train_agg_df = train_df.groupby('ID').agg(agg_ops).reset_index()\n",
    "train_agg_df.rename(columns={'weekly_demand': 'total_demand', 'weekly_sales': 'total_sales'}, inplace=True)\n",
    "train_agg_df['sell_through'] = train_agg_df['total_sales'] / (train_agg_df['Production'] + 1e-6) \n",
    "train_agg_df['sell_through'] = train_agg_df['sell_through'].fillna(0)\n",
    "train_agg_df['is_stockout'] = (train_agg_df['sell_through'] >= 0.98).astype(int)\n",
    "\n",
    "# --- 2.1 PREPROCESSING: Eliminar filas con missings críticos ---\n",
    "print(\"Paso 2.1: Eliminando filas con valores faltantes en columnas clave...\")\n",
    "cols_critical = ['Production', 'image_embedding']  # Columnas que no pueden tener NaN\n",
    "train_agg_df = train_agg_df.dropna(subset=[c for c in cols_critical if c in train_agg_df.columns])\n",
    "print(f\"Filas después de eliminar missings críticos: {len(train_agg_df)} (eliminadas: {original_train_len - len(train_agg_df)})\")\n",
    "\n",
    "# --- 3. Procesamiento de Embeddings (k-NN) ---\n",
    "print(\"Paso 3: Procesando embeddings y creando features k-NN...\")\n",
    "train_agg_df['embedding_vec'] = train_agg_df['image_embedding'].apply(parse_embedding)\n",
    "test_df['embedding_vec'] = test_df['image_embedding'].apply(parse_embedding)\n",
    "X_train_embeddings = np.stack(train_agg_df['embedding_vec'].values)\n",
    "X_test_embeddings = np.stack(test_df['embedding_vec'].values)\n",
    "knn_model = NearestNeighbors(n_neighbors=N_NEIGHBORS, metric='cosine', n_jobs=-1)\n",
    "knn_model.fit(X_train_embeddings)\n",
    "distances, indices = knn_model.kneighbors(X_test_embeddings)\n",
    "cols_to_fetch = {\n",
    "    'total_demand': 'total_demand', 'total_sales': 'total_sales', 'Production': 'Production', \n",
    "    'num_stores': 'num_stores', 'sell_through': 'sell_through', 'is_stockout': 'is_stockout'\n",
    "}\n",
    "for col_name_in_train, new_feature_prefix in cols_to_fetch.items():\n",
    "    if col_name_in_train in train_agg_df.columns:\n",
    "        neighbor_values = train_agg_df.iloc[indices.flatten()][col_name_in_train].values\n",
    "        neighbor_values = neighbor_values.reshape(len(test_df), N_NEIGHBORS)\n",
    "        test_df[f'avg_neighbor_{new_feature_prefix}'] = np.mean(neighbor_values, axis=1)\n",
    "\n",
    "# --- 4. Pre-procesamiento Final (LGBM) ---\n",
    "print(\"Paso 4: Pre-procesamiento final - One-Hot Encoding y Escalado...\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "special_cols = ['image_embedding', 'embedding_vec', 'phase_in', 'phase_out', 'color_rgb', 'total_demand', 'total_sales', 'ID']\n",
    "all_categorical = list(set(train_agg_df.select_dtypes(include=['object']).columns) | set(test_df.select_dtypes(include=['object']).columns))\n",
    "all_categorical = [col for col in all_categorical if col not in special_cols]\n",
    "\n",
    "train_shape = len(train_agg_df)\n",
    "combined_df = pd.concat([train_agg_df.drop(['total_demand', 'total_sales'], axis=1, errors='ignore').copy(), test_df.copy()], ignore_index=True)\n",
    "\n",
    "# Preprocesar fechas y básicos\n",
    "combined_df = preprocess_features_lgbm(combined_df, all_categorical)\n",
    "\n",
    "# --- 4.1 ONE-HOT ENCODING ---\n",
    "print(f\"Aplicando One-Hot Encoding a {len(all_categorical)} columnas categóricas...\")\n",
    "# Convertir categóricas a strings primero\n",
    "for col in all_categorical:\n",
    "    if col in combined_df.columns:\n",
    "        combined_df[col] = combined_df[col].astype(str)\n",
    "\n",
    "# One-hot encoding\n",
    "combined_df = pd.get_dummies(combined_df, columns=all_categorical, drop_first=True, dtype=int)\n",
    "\n",
    "# --- LIMPIEZA DE NOMBRES DE COLUMNAS PARA LIGHTGBM ---\n",
    "# LightGBM no acepta caracteres especiales JSON en nombres\n",
    "combined_df.columns = combined_df.columns.str.replace('[', '_', regex=False)\n",
    "combined_df.columns = combined_df.columns.str.replace(']', '_', regex=False)\n",
    "combined_df.columns = combined_df.columns.str.replace('<', '_', regex=False)\n",
    "combined_df.columns = combined_df.columns.str.replace('>', '_', regex=False)\n",
    "combined_df.columns = combined_df.columns.str.replace('{', '_', regex=False)\n",
    "combined_df.columns = combined_df.columns.str.replace('}', '_', regex=False)\n",
    "combined_df.columns = combined_df.columns.str.replace('\"', '_', regex=False)\n",
    "combined_df.columns = combined_df.columns.str.replace(\"'\", '_', regex=False)\n",
    "combined_df.columns = combined_df.columns.str.replace(':', '_', regex=False)\n",
    "combined_df.columns = combined_df.columns.str.replace(',', '_', regex=False)\n",
    "combined_df.columns = combined_df.columns.str.replace(' ', '_', regex=False)\n",
    "\n",
    "# Separar nuevamente train y test\n",
    "train_processed_df = combined_df.iloc[:train_shape].copy()\n",
    "test_processed_df = combined_df.iloc[train_shape:].copy()\n",
    "train_processed_df = train_processed_df.assign(total_demand=train_agg_df['total_demand'].values)\n",
    "\n",
    "# --- 4.2 ESCALADO DE DATOS NUMÉRICOS ---\n",
    "print(\"Escalando features numéricas con StandardScaler...\")\n",
    "target = 'total_demand'\n",
    "features = [col for col in train_processed_df.columns if col not in special_cols and col != target]\n",
    "\n",
    "# Identificar columnas numéricas (excluyendo one-hot encoded que son 0/1)\n",
    "numeric_cols = train_processed_df[features].select_dtypes(include=[np.number]).columns.tolist()\n",
    "# Filtrar one-hot (típicamente tienen valores solo 0 y 1)\n",
    "numeric_cols_to_scale = [c for c in numeric_cols if train_processed_df[c].nunique() > 2]\n",
    "\n",
    "if numeric_cols_to_scale:\n",
    "    scaler = StandardScaler()\n",
    "    train_processed_df[numeric_cols_to_scale] = scaler.fit_transform(train_processed_df[numeric_cols_to_scale])\n",
    "    test_processed_df[numeric_cols_to_scale] = scaler.transform(test_processed_df[numeric_cols_to_scale])\n",
    "    print(f\"Escaladas {len(numeric_cols_to_scale)} columnas numéricas\")\n",
    "\n",
    "# --- 5. Definir Features y Target ---\n",
    "print(\"Paso 5: Definiendo features y target...\")\n",
    "target = 'total_demand'\n",
    "features = [col for col in train_processed_df.columns if col not in special_cols and col != target]\n",
    "# Ya no necesitamos categorical features porque hicimos one-hot encoding\n",
    "X_train_final = train_processed_df[features]\n",
    "y_train_final = train_processed_df[target]\n",
    "X_test_final = test_processed_df[features]\n",
    "X_test_final = X_test_final[X_train_final.columns]\n",
    "print(f\"Usando {len(features)} features (incluyendo one-hot encoded).\")\n",
    "\n",
    "# --- 6. Entrenar Modelo FINAL (con 100% de datos) ---\n",
    "print(f\"Paso 6: Entrenando LightGBM final con 100% de datos...\")\n",
    "print(f\"Parámetros: objective='regression_l1', n_estimators={N_ESTIMATORS_OPTIMO}\")\n",
    "lgbm_params = {\n",
    "    'objective': 'regression_l1', 'metric': 'mae',\n",
    "    'n_estimators': N_ESTIMATORS_OPTIMO, 'learning_rate': 0.05,\n",
    "    'n_jobs': -1, 'random_state': 42, 'verbose': -1\n",
    "}\n",
    "model_final = lgb.LGBMRegressor(**lgbm_params)\n",
    "# Sin categorical_feature porque ya hicimos one-hot encoding\n",
    "model_final.fit(X_train_final, y_train_final)\n",
    "print(\"Entrenamiento final completado.\")\n",
    "\n",
    "# --- 7. Generar \"Fábrica\" de Submissions ---\n",
    "print(\"Paso 7: Generando predicciones base (MAE)...\")\n",
    "# Generar UNA SOLA VEZ las predicciones de \"precisión\"\n",
    "base_predictions = model_final.predict(X_test_final)\n",
    "base_predictions[base_predictions < 0] = 0 # Limpiar negativos\n",
    "\n",
    "print(f\"¡Generando {len(RISK_FACTORS)} archivos de submission!\")\n",
    "for (name, factor) in RISK_FACTORS.items():\n",
    "    \n",
    "    # Aplicar el \"Factor de Riesgo\"\n",
    "    final_predictions = base_predictions * factor\n",
    "    \n",
    "    submission_df = pd.DataFrame({'ID': test_df['ID'], 'Production': final_predictions})\n",
    "    submission_df['Production'] = submission_df['Production'].round().astype(int)\n",
    "    \n",
    "    filename = f\"submission_{name}_factor_{factor:.2f}.csv\"\n",
    "    submission_df.to_csv(filename, index=False)\n",
    "    \n",
    "    print(f\"Archivo guardado: {filename}\")\n",
    "    print(submission_df.head())\n",
    "    print(\"---\")\n",
    "print(f\"{len(RISK_FACTORS)} archivos de submission generados. ¡Súbelos y compara los scores!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
